{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Distiller Post-Train Quantization Models to \"Native\" PyTorch\n",
    "\n",
    "## Background\n",
    "\n",
    "As of version 1.3 PyTorch comes with built-in quantization functionality. Details are available [here](https://pytorch.org/docs/stable/quantization.html). Distiller's and PyTorch's implementations are completely unrelated. An advantage of PyTorch built-in quantization is that it offers optimized 8-bit execution on CPU and export to GLOW. PyTorch doesn't offer optimized 8-bit execution on GPU (as of version 1.4).\n",
    "\n",
    "At the moment we are still keeping Distiller's separate API and implementation, but we've added the capability to convert a **post-training quantization** model created in Distiller to a \"Distiller-free\" model, comprised entirely of PyTorch built-in quantized modules.\n",
    "\n",
    "Distiller's quantized layers are actually simulated in FP32. Hence, comparing a Distiller model running on CPU to a PyTorch built-in model, the latter will be significantly faster on CPU. However, a Distiller model on a GPU is still likely to be faster compared to a PyTorch model on CPU. So experimenting with Distiller and converting to PyTorch in the end could be useful. Milage may vary of course, depending on the actual HW setup.\n",
    "\n",
    "Let's see how the conversion works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logging configured successfully\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import torchnet as tnt\n",
    "from ipywidgets import widgets, interact\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import distiller\n",
    "from distiller.models import create_model\n",
    "import distiller.quantization as quant\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# Load some common code and configure logging\n",
    "# We do this so we can see the logging output coming from\n",
    "# Distiller function calls\n",
    "%run './distiller_jupyter_helpers.ipynb'\n",
    "msglogger = config_notebooks_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> created a pretrained resnet18 model with the imagenet dataset\n"
     ]
    }
   ],
   "source": [
    "# By default, the model is moved to the GPU and parallelized (wrapped with torch.nn.DataParallel)\n",
    "# If no GPU is available, a non-parallel model is created on the CPU\n",
    "model = create_model(pretrained=True, dataset='imagenet', arch='resnet18', parallel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Loaders\n",
    "\n",
    "We create separate data loaders for GPU and CPU. Set `batch_size` and `num_workers` to optimal values that match your HW setup.\n",
    "\n",
    "(Note we reset the seed before creating each data loader, to make sure both loaders consist of the same subset of the test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# We use Distiller's built-in data loading functionality for ImageNet\n",
    "\n",
    "distiller.set_seed(0)\n",
    "\n",
    "subset_size = 1.0 # To save time, can set to value < 1.0\n",
    "dataset = 'cifar10'\n",
    "dataset_path = os.path.expanduser('../../data.cifar10')\n",
    "\n",
    "batch_size_gpu = 256\n",
    "num_workers_gpu = 10\n",
    "_, _, test_loader_gpu, _ = distiller.apputils.load_data(\n",
    "    dataset, dataset_path, batch_size_gpu, num_workers_gpu,\n",
    "    effective_test_size=subset_size, fixed_subset=True, test_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "distiller.set_seed(0)\n",
    "batch_size_cpu = 44\n",
    "num_workers_cpu = 10\n",
    "_, _, test_loader_cpu, _ = distiller.apputils.load_data(\n",
    "    dataset, dataset_path, batch_size_cpu, num_workers_cpu,\n",
    "    effective_test_size=subset_size, fixed_subset=True, test_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(data_loader, model, device, print_freq=10):\n",
    "    print('Evaluating model')\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    loss = tnt.meter.AverageValueMeter()\n",
    "    classerr = tnt.meter.ClassErrorMeter(accuracy=True, topk=(1, 5))\n",
    "\n",
    "    total_samples = len(data_loader.sampler)\n",
    "    batch_size = data_loader.batch_size\n",
    "    total_steps = math.ceil(total_samples / batch_size)\n",
    "    print('{0} samples ({1} per mini-batch)'.format(total_samples, batch_size))\n",
    "\n",
    "    # Switch to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    for step, (inputs, target) in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            # compute output from model\n",
    "            output = model(inputs)\n",
    "\n",
    "            # compute loss and measure accuracy\n",
    "            loss.add(criterion(output, target).item())\n",
    "            classerr.add(output.data, target)\n",
    "            \n",
    "            if (step + 1) % print_freq == 0:\n",
    "                print('[{:3d}/{:3d}] Top1: {:.3f}  Top5: {:.3f}  Loss: {:.3f}'.format(\n",
    "                      step + 1, total_steps, classerr.value(1), classerr.value(5), loss.mean), flush=True)\n",
    "    print('----------')\n",
    "    print('Overall ==> Top1: {:.3f}  Top5: {:.3f}  Loss: {:.3f}'.format(\n",
    "        classerr.value(1), classerr.value(5), loss.mean), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Train Quantize with Distiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading activation stats from: ../examples/quantization/post_train_quant/stats/resnet18_quant_stats.yaml\n",
      "Preparing model for quantization using PostTrainLinearQuantizer\n",
      "Applying batch-norm folding ahead of post-training quantization\n",
      "Propagating output statistics from BN modules to folded modules\n",
      "Optimizing output statistics for modules followed by ReLU/Tanh/Sigmoid\n",
      "Updated stats saved to ./quant_stats_after_prepare_model.yaml\n",
      "Per-layer quantization parameters saved to ./layer_quant_params.yaml\n"
     ]
    }
   ],
   "source": [
    "quant_mode = {'activations': 'ASYMMETRIC_UNSIGNED', 'weights': 'SYMMETRIC'}\n",
    "stats_file = \"../examples/quantization/post_train_quant/stats/resnet18_quant_stats.yaml\"\n",
    "dummy_input = distiller.get_dummy_input(input_shape=model.input_shape)\n",
    "\n",
    "quantizer = quant.PostTrainLinearQuantizer(\n",
    "    deepcopy(model), bits_activations=8, bits_parameters=8, mode=quant_mode,\n",
    "    model_activation_stats=stats_file, overrides=None\n",
    ")\n",
    "quantizer.prepare_model(dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to PyTorch Built-In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distiller model device: cuda:0\n",
      "PyTorch model device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Here we trigger the conversion via the Quantizer instance. Later on we show another way which does not\n",
    "# require the quantizer\n",
    "pyt_model = quantizer.convert_to_pytorch(dummy_input)\n",
    "\n",
    "# Note that the converted model is automatically moved to the CPU, regardless\n",
    "# of the device of the Distiller model\n",
    "print('Distiller model device:', distiller.model_device(quantizer.model))\n",
    "print('PyTorch model device:', distiller.model_device(pyt_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "### Distiller Model on GPU (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "10000 samples (256 per mini-batch)\n",
      "[ 10/ 40] Top1: 0.000  Top5: 0.313  Loss: 8.699\n",
      "[ 20/ 40] Top1: 0.000  Top5: 0.332  Loss: 8.709\n",
      "[ 30/ 40] Top1: 0.013  Top5: 0.273  Loss: 8.703\n",
      "[ 40/ 40] Top1: 0.030  Top5: 0.290  Loss: 8.669\n",
      "----------\n",
      "Overall ==> Top1: 0.030  Top5: 0.290  Loss: 8.669\n",
      "CPU times: user 16.7 s, sys: 622 ms, total: 17.3 s\n",
      "Wall time: 1.65 s\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    %time eval_model(test_loader_gpu, quantizer.model, 'cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distiller Model on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CPU copy of Distiller model\n",
      "Evaluating model\n",
      "10000 samples (44 per mini-batch)\n",
      "[ 60/228] Top1: 0.000  Top5: 0.265  Loss: 8.669\n",
      "[120/228] Top1: 0.019  Top5: 0.303  Loss: 8.682\n",
      "[180/228] Top1: 0.038  Top5: 0.328  Loss: 8.682\n",
      "----------\n",
      "Overall ==> Top1: 0.030  Top5: 0.290  Loss: 8.681\n",
      "CPU times: user 2min 9s, sys: 3.57 s, total: 2min 12s\n",
      "Wall time: 6.35 s\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('Creating CPU copy of Distiller model')\n",
    "    cpu_model = distiller.make_non_parallel_copy(quantizer.model).cpu()\n",
    "else:\n",
    "    cpu_model = quantizer.model\n",
    "%time eval_model(test_loader_cpu, cpu_model, 'cpu', print_freq=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch model in CPU\n",
    "\n",
    "We expect the PyTorch model on CPU to be much faster than the Distiller model on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "10000 samples (44 per mini-batch)\n",
      "[ 60/228] Top1: 0.000  Top5: 0.492  Loss: 8.672\n",
      "[120/228] Top1: 0.019  Top5: 0.473  Loss: 8.692\n",
      "[180/228] Top1: 0.051  Top5: 0.379  Loss: 8.706\n",
      "----------\n",
      "Overall ==> Top1: 0.040  Top5: 0.340  Loss: 8.716\n",
      "CPU times: user 46.5 s, sys: 2.49 s, total: 49 s\n",
      "Wall time: 2.67 s\n"
     ]
    }
   ],
   "source": [
    "%time eval_model(test_loader_cpu, pyt_model, 'cpu', print_freq=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the Extra-Curious: Comparing the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Distiller takes care of quantizing the inputs within the quantized modules PyTorch quantized modules assume the input is already quantized. Hence, for cases where a module's input is not quantized, we explicitly add a quantization operation for the input. The first layer in the model, `conv1` in ResNet18, is such a case\n",
    "2. Both Distiller and native PyTorch support fused ReLU. In Distiller, this is somewhat obscurely indicated by the `clip_half_range` attribute inside `output_quant_settings`. In PyTorch, the module type is explicitly `QuantizedConvReLU2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1\n",
      "\n",
      "DISTILLER:\n",
      "RangeLinearQuantParamLayerWrapper(\n",
      "  weights_quant_settings=(num_bits=8 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)\n",
      "  output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)\n",
      "  accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)\n",
      "    inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None\n",
      "  scale_approx_mult_bits=None\n",
      "  preset_activation_stats=True\n",
      "    output_scale=34.659176, output_zero_point=0.000000\n",
      "  weights_scale=323.706360, weights_zero_point=0.000000\n",
      "  (wrapped_module): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      ")\n",
      "\n",
      "PyTorch:\n",
      "ConditionalQuantizeWrapper(\n",
      "  (quant): ConditionalQuantize(\n",
      "    (quantizers): ModuleDict(\n",
      "      (0): Quantize(scale=tensor([0.0375]), zero_point=tensor([57]), dtype=torch.quint8)\n",
      "    )\n",
      "  )\n",
      "  (wrapped): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.05793195962905884, zero_point=0, padding=(3, 3))\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('conv1\\n')\n",
    "print('DISTILLER:\\n{}\\n'.format(quantizer.model.module.conv1))\n",
    "print('PyTorch:\\n{}\\n'.format(pyt_model.conv1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of internal layers which don't require explicit input quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.0.conv1\n",
      "QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.03515824303030968, zero_point=0, padding=(1, 1))\n",
      "\n",
      "layer1.0.add\n",
      "QFunctionalAddRelu(\n",
      "  (qfunc): QFunctional()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('layer1.0.conv1')\n",
    "print(pyt_model.layer1[0].conv1)\n",
    "print('\\nlayer1.0.add')\n",
    "print(pyt_model.layer1[0].add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic de-quantization <--> quantization in the model\n",
    "\n",
    "For each quantized module in the Distiller implementation, we quantize the input and de-quantize the output.\n",
    "So, if the user explicitly sets \"internal\" modules to run in FP32, this is transparent to the other quantized modules (at the cost of redundant quant-dequant operations).\n",
    "\n",
    "When converting to PyTorch we remove these redundant operations, and keep just the required ones in case the user explicitly decided to run some modules in FP32.\n",
    "\n",
    "For an example, consider a ResNet \"basic block\" with a residual connection that contains a downsampling convolution. Let's see how such a block looks in our fully-quantized, converted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistillerBasicBlock(\n",
      "  (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.033819135278463364, zero_point=0, padding=(1, 1))\n",
      "  (bn1): Identity()\n",
      "  (relu1): Identity()\n",
      "  (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.09309203922748566, zero_point=49, padding=(1, 1))\n",
      "  (bn2): Identity()\n",
      "  (downsample): Sequential(\n",
      "    (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.08124598860740662, zero_point=68)\n",
      "    (1): Identity()\n",
      "  )\n",
      "  (add): QFunctionalAddRelu(\n",
      "    (qfunc): QFunctional()\n",
      "  )\n",
      "  (relu2): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pyt_model.layer2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see all layers are either built-in quantized PyTorch modules, or identity operations representing fused operations. The entire block is quantized, so we don't see any quant-dequnt operations in the middle.\n",
    "\n",
    "Now let's create a new quantized model, and this time leave the 'downsample' module in FP32:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading activation stats from: ../examples/quantization/post_train_quant/stats/resnet18_quant_stats.yaml\n",
      "Preparing model for quantization using PostTrainLinearQuantizer\n",
      "Applying batch-norm folding ahead of post-training quantization\n",
      "Propagating output statistics from BN modules to folded modules\n",
      "Optimizing output statistics for modules followed by ReLU/Tanh/Sigmoid\n",
      "Updated stats saved to ./quant_stats_after_prepare_model.yaml\n",
      "Per-layer quantization parameters saved to ./layer_quant_params.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistillerBasicBlock(\n",
      "  (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.033819135278463364, zero_point=0, padding=(1, 1))\n",
      "  (bn1): Identity()\n",
      "  (relu1): Identity()\n",
      "  (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.09309203922748566, zero_point=49, padding=(1, 1))\n",
      "  (bn2): Identity()\n",
      "  (downsample): Sequential(\n",
      "    (0): ConditionalDeQuantizeWrapper(\n",
      "      (dequant): ConditionalDeQuantize()\n",
      "      (wrapped): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "    )\n",
      "    (1): Identity()\n",
      "  )\n",
      "  (add): ConditionalQuantizeWrapper(\n",
      "    (quant): ConditionalQuantize(\n",
      "      (quantizers): ModuleDict(\n",
      "        (1): Quantize(scale=tensor([0.0812]), zero_point=tensor([68]), dtype=torch.quint8)\n",
      "      )\n",
      "    )\n",
      "    (wrapped): QFunctionalAddRelu(\n",
      "      (qfunc): QFunctional()\n",
      "    )\n",
      "  )\n",
      "  (relu2): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "overrides = OrderedDict(\n",
    "    [('layer2.0.downsample.0', OrderedDict([('bits_activations', None), ('bits_weights', None)]))]\n",
    ")\n",
    "new_quantizer = quant.PostTrainLinearQuantizer(\n",
    "    deepcopy(model), bits_activations=8, bits_parameters=8, mode=quant_mode,\n",
    "    model_activation_stats=stats_file, overrides=overrides\n",
    ")\n",
    "new_quantizer.prepare_model(dummy_input)\n",
    "\n",
    "new_pyt_model = new_quantizer.convert_to_pytorch(dummy_input)\n",
    "\n",
    "print(new_pyt_model.layer2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a few differences:\n",
    "1. The `downsample` module now contains a de-quantize op before the actual convolution\n",
    "2. The `add` module now contains a quantize op before the actual add. Note that the add operation accepts 2 inputs. In this case the first input (index 0) comes from the `conv2` module, which is quantized. The second input (index 1) comes from the `downsample` module, which we kept in FP32. So, we only need to quantized the input at index 1. We can see this is indeed what is happening, by looking at the `ModuleDict` inside the `quant` module, and noticing it has only a single key for index \"1\".\n",
    "\n",
    "Let's see how the `add` module would look if we also kept the `conv2` module in FP32:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading activation stats from: ../examples/quantization/post_train_quant/stats/resnet18_quant_stats.yaml\n",
      "Preparing model for quantization using PostTrainLinearQuantizer\n",
      "Applying batch-norm folding ahead of post-training quantization\n",
      "Propagating output statistics from BN modules to folded modules\n",
      "Optimizing output statistics for modules followed by ReLU/Tanh/Sigmoid\n",
      "Updated stats saved to ./quant_stats_after_prepare_model.yaml\n",
      "Per-layer quantization parameters saved to ./layer_quant_params.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConditionalQuantizeWrapper(\n",
      "  (quant): ConditionalQuantize(\n",
      "    (quantizers): ModuleDict(\n",
      "      (0): Quantize(scale=tensor([0.0931]), zero_point=tensor([49]), dtype=torch.quint8)\n",
      "      (1): Quantize(scale=tensor([0.0812]), zero_point=tensor([68]), dtype=torch.quint8)\n",
      "    )\n",
      "  )\n",
      "  (wrapped): QFunctionalAddRelu(\n",
      "    (qfunc): QFunctional()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "overrides = OrderedDict(\n",
    "    [('layer2.0.downsample.0', OrderedDict([('bits_activations', None), ('bits_weights', None)])),\n",
    "     ('layer2.0.conv2', OrderedDict([('bits_activations', None), ('bits_weights', None)]))]\n",
    ")\n",
    "new_quantizer = quant.PostTrainLinearQuantizer(\n",
    "    deepcopy(model), bits_activations=8, bits_parameters=8, mode=quant_mode,\n",
    "    model_activation_stats=stats_file, overrides=overrides\n",
    ")\n",
    "new_quantizer.prepare_model(dummy_input)\n",
    "\n",
    "new_pyt_model = new_quantizer.convert_to_pytorch(dummy_input)\n",
    "\n",
    "print(new_pyt_model.layer2[0].add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that now both inputs to the add module are being quantized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another API for Conversion\n",
    "\n",
    "In some cases we don't have the actual quantizer. For example - if the Distiller quantized module was loaded from a checkpoint. In those cases we can call a `distiller.quantization` module-level function (In fact, the Quantizer method we used earlier is a wrapper around this function).\n",
    "\n",
    "### Save Distiller model to checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving checkpoint to: ./checkpoint.pth.tar\n"
     ]
    }
   ],
   "source": [
    "# Save Distiller model to checkpoint and load it\n",
    "distiller.apputils.save_checkpoint(0, 'resnet18', quantizer.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Checkpoint\n",
    "\n",
    "The model is quantized when the checkpoint is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> created a resnet18 model with the imagenet dataset\n",
      "=> loading checkpoint checkpoint.pth.tar\n",
      "=> Checkpoint contents:\n",
      "+--------------------+-------------+----------+\n",
      "| Key                | Type        | Value    |\n",
      "|--------------------+-------------+----------|\n",
      "| arch               | str         | resnet18 |\n",
      "| dataset            | str         | imagenet |\n",
      "| epoch              | int         | 0        |\n",
      "| extras             | dict        |          |\n",
      "| is_parallel        | bool        | True     |\n",
      "| quantizer_metadata | dict        |          |\n",
      "| state_dict         | OrderedDict |          |\n",
      "+--------------------+-------------+----------+\n",
      "\n",
      "=> Checkpoint['extras'] contents:\n",
      "+-------+--------+---------+\n",
      "| Key   | Type   | Value   |\n",
      "|-------+--------+---------|\n",
      "+-------+--------+---------+\n",
      "\n",
      "Warning: compression schedule data does not exist in the checkpoint\n",
      "Loaded quantizer metadata from the checkpoint\n",
      "Preparing model for quantization using PostTrainLinearQuantizer\n",
      "Applying batch-norm folding ahead of post-training quantization\n",
      "Propagating output statistics from BN modules to folded modules\n",
      "Optimizing output statistics for modules followed by ReLU/Tanh/Sigmoid\n",
      "Updated stats saved to ./quant_stats_after_prepare_model.yaml\n",
      "Per-layer quantization parameters saved to ./layer_quant_params.yaml\n",
      "=> loaded 'state_dict' from checkpoint 'checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "loaded_model = create_model(False, dataset='imagenet', arch='resnet18', parallel=True)\n",
    "loaded_model = distiller.apputils.load_lean_checkpoint(loaded_model, 'checkpoint.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "10000 samples (44 per mini-batch)\n",
      "[ 60/228] Top1: 0.114  Top5: 0.530  Loss: 8.681\n",
      "[120/228] Top1: 0.076  Top5: 0.341  Loss: 8.720\n",
      "[180/228] Top1: 0.076  Top5: 0.341  Loss: 8.712\n",
      "----------\n",
      "Overall ==> Top1: 0.070  Top5: 0.330  Loss: 8.720\n",
      "CPU times: user 45.1 s, sys: 2.03 s, total: 47.1 s\n",
      "Wall time: 2.6 s\n"
     ]
    }
   ],
   "source": [
    "# Convert\n",
    "loaded_pyt_model = distiller.quantization.convert_distiller_ptq_model_to_pytorch(loaded_model, dummy_input)\n",
    "\n",
    "# Run evaluation\n",
    "%time eval_model(test_loader_cpu, loaded_pyt_model, 'cpu', print_freq=60)\n",
    "\n",
    "# Cleanup\n",
    "os.remove('checkpoint.pth.tar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dist_test2",
   "language": "python",
   "name": "dist_test2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
