{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "import distiller\n",
    "from distiller.data_loggers import collector_context\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 1\n",
    "batch_size = 4\n",
    "model_type = 3\n",
    "\n",
    "model_table = {1: 'torch_scratch',      # not working\n",
    "        2: 'torchvision_pretrained',    # not working\n",
    "        3: 'distiller_not_parallel',\n",
    "        4: 'distiller_parallel'         # not working\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Helper functions\n",
    "#####################################\n",
    "def save_model_structure(path, filename, model):\n",
    "    with open(path+filename, 'w') as text_file:\n",
    "        text_file.write(str(model))\n",
    "    return\n",
    "\n",
    "def generate_model_info(model, model_name):\n",
    "    print('- %s, type:%s' %(model_name, type(model)))\n",
    "\n",
    "    path_structure = './model_structure/'\n",
    "    save_model_structure(path_structure, model_name+'.txt', model)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      " ship   car   dog  bird\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# 1. Load and normalizing the CIFAR10 training and test datasets using\n",
    "# torchvision\n",
    "########################################################################\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "path_c10 = '../../data.cifar10/'\n",
    "trainset = torchvision.datasets.CIFAR10(root=path_c10, train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=path_c10, train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- distiller_not_parallel, type:<class 'torchvision.models.resnet.ResNet'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): DistillerBasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (add): EltwiseAdd()\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): DistillerBasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (add): EltwiseAdd()\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): DistillerBasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (add): EltwiseAdd()\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): DistillerBasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (add): EltwiseAdd()\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): DistillerBasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (add): EltwiseAdd()\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): DistillerBasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (add): EltwiseAdd()\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): DistillerBasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (add): EltwiseAdd()\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): DistillerBasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (add): EltwiseAdd()\n",
       "      (relu2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################################################\n",
    "# 2. Define a network\n",
    "########################################################################\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "from distiller.models import create_model\n",
    "import distiller.quantization as quant\n",
    "from copy import deepcopy\n",
    "\n",
    "if model_table[model_type] == 'torch_scratch':\n",
    "    model = Net()\n",
    "elif model_table[model_type] == 'distiller_not_parallel':\n",
    "    model= create_model(pretrained=True, dataset='imagenet', arch='resnet18', parallel=False)\n",
    "elif model_table[model_type] == 'distiller_not_parallel':\n",
    "    model= create_model(pretrained=True, dataset='imagenet', arch='resnet18', parallel=True)\n",
    "    \n",
    "generate_model_info(model, model_table[model_type])\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# 3. Define a loss function and optimizer\n",
    "########################################################################\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.902, time:29.8\n",
      "[1,  4000] loss: 2.656, time:29.7\n",
      "[1,  6000] loss: 2.592, time:30.5\n",
      "[1,  8000] loss: 2.506, time:29.7\n",
      "[1, 10000] loss: 2.520, time:30.8\n",
      "[1, 12000] loss: 2.450, time:30.2\n",
      "\t val_loss: 2.388, acc:22.35%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# 4. Train the network\n",
    "########################################################################\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            end = time.time()\n",
    "            print('[%d, %5d] loss: %.3f, time:%.1f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000, end-start))\n",
    "            running_loss = 0.0\n",
    "            start = time.time()\n",
    "\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(testloader)\n",
    "\n",
    "        print('\\t val_loss: %.3f, acc:%.2f%%' %\n",
    "                (val_loss, 100. * correct / total))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[eval] loss:2.388, acc:22.35%\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# 5. generate stats\n",
    "########################################################################\n",
    "path_yaml = './stat_yaml/'\n",
    "stat_filename = 'acts_quantization_stats.yaml'\n",
    "\n",
    "# CHECK: /examples/word_language_model/quantize_lstm.ipynb\n",
    "def evaluate(model):\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(testloader)\n",
    "\n",
    "    eval_acc = 100 * correct / total\n",
    "\n",
    "    return val_loss, eval_acc\n",
    "\n",
    "eval_loss, eval_acc = evaluate(model)\n",
    "print('[eval] loss:%.3f, acc:%.2f%%' % (eval_loss, eval_acc))\n",
    "\n",
    "def test_fn(model):\n",
    "    return evaluate(model)[0]\n",
    "\n",
    "from distiller.data_loggers import collect_quant_stats\n",
    "collect_quant_stats(model, test_fn, save_dir=path_yaml)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Quantization\n",
    "#####################################\n",
    "\n",
    "quant_mode = {'activations': 'ASYMMETRIC_UNSIGNED', 'weights': 'SYMMETRIC'}\n",
    "stats_file = path_yaml + stat_filename\n",
    "#stats_file = \"../quantization/post_train_quant/stats/resnet18_quant_stats.yaml\"\n",
    "#dummy_input = distiller.get_dummy_input(dataset='cifar10')\n",
    "dummy_input = distiller.get_dummy_input(input_shape=model.input_shape)\n",
    "\n",
    "quantizer = quant.PostTrainLinearQuantizer(\n",
    "    deepcopy(model), bits_activations=8, bits_parameters=8, mode=quant_mode,\n",
    "    model_activation_stats=stats_file, overrides=None\n",
    ")\n",
    "quantizer.prepare_model(dummy_input)\n",
    "\n",
    "pyt_model = quantizer.convert_to_pytorch(dummy_input)\n",
    "generate_model_info(pyt_model, 'after_quant')\n",
    "\n",
    "print('Distiller model device:', distiller.model_device(quantizer.model))\n",
    "print('PyTorch model device:', distiller.model_device(pyt_model))\n",
    "\n",
    "print(pyt_model.layer1[0].conv1.weight().int_repr().data[0, 0, :, :])\n",
    "print(pyt_model.layer1[0].conv1.weight().dequantize().data[0, 0, :, :])\n",
    "\n",
    "\n",
    "print('DISTILLER1:\\n{}\\n'.format(quantizer.model.conv1))\n",
    "#print('DISTILLER2:\\n{}\\n'.format(quantizer.model.module.conv1))\n",
    "print('PyTorch:\\n{}\\n'.format(pyt_model.conv1))\n",
    "\n",
    "print('layer1.0.conv1')\n",
    "print(pyt_model.layer1[0].conv1)\n",
    "print('\\nlayer1.0.add')\n",
    "print(pyt_model.layer1[0].add)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use Distiller's built-in data loading functionality for ImageNet\n",
    "\n",
    "distiller.set_seed(0)\n",
    "\n",
    "subset_size = 1.0 # To save time, can set to value < 1.0\n",
    "dataset = 'cifar10'\n",
    "dataset_path = os.path.expanduser('../../data.cifar10')\n",
    "\n",
    "batch_size_gpu = 256\n",
    "num_workers_gpu = 10\n",
    "_, _, test_loader_gpu, _ = distiller.apputils.load_data(\n",
    "    dataset, dataset_path, batch_size_gpu, num_workers_gpu,\n",
    "    effective_test_size=subset_size, fixed_subset=True, test_only=True)\n",
    "\n",
    "distiller.set_seed(0)\n",
    "batch_size_cpu = 44\n",
    "num_workers_cpu = 10\n",
    "_, _, test_loader_cpu, _ = distiller.apputils.load_data(\n",
    "    dataset, dataset_path, batch_size_cpu, num_workers_cpu,\n",
    "    effective_test_size=subset_size, fixed_subset=True, test_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchnet as tnt\n",
    "\n",
    "def eval_model(data_loader, model, device, print_freq=10):\n",
    "    print('Evaluating model')\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    loss = tnt.meter.AverageValueMeter()\n",
    "    classerr = tnt.meter.ClassErrorMeter(accuracy=True, topk=(1, 5))\n",
    "\n",
    "    total_samples = len(data_loader.sampler)\n",
    "    batch_size = data_loader.batch_size\n",
    "    total_steps = math.ceil(total_samples / batch_size)\n",
    "    print('{0} samples ({1} per mini-batch)'.format(total_samples, batch_size))\n",
    "\n",
    "    # Switch to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    for step, (inputs, target) in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            # compute output from model\n",
    "            output = model(inputs)\n",
    "\n",
    "            # compute loss and measure accuracy\n",
    "            loss.add(criterion(output, target).item())\n",
    "            classerr.add(output.data, target)\n",
    "\n",
    "            if (step + 1) % print_freq == 0:\n",
    "                print('[{:3d}/{:3d}] Top1: {:.3f}  Top5: {:.3f}  Loss: {:.3f}'.format(\n",
    "                      step + 1, total_steps, classerr.value(1), classerr.value(5), loss.mean), flush=True)\n",
    "    print('----------')\n",
    "    print('Overall ==> Top1: {:.3f}  Top5: {:.3f}  Loss: {:.3f}'.format(\n",
    "        classerr.value(1), classerr.value(5), loss.mean), flush=True)\n",
    "\n",
    "    return\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    eval_model(test_loader_gpu, quantizer.model, 'cuda')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Creating CPU copy of Distiller model')\n",
    "    cpu_model = distiller.make_non_parallel_copy(quantizer.model).cpu()\n",
    "else:\n",
    "    cpu_model = quantizer.model\n",
    "eval_model(test_loader_cpu, cpu_model, 'cpu', print_freq=60)\n",
    "\n",
    "\n",
    "eval_model(test_loader_cpu, pyt_model, 'cpu', print_freq=60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    eval_model(test_loader_gpu, quantizer.model, 'cuda')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dist_test2",
   "language": "python",
   "name": "dist_test2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
